{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4629ddc"
      },
      "source": [
        "# Retrieval Augmented Generation (RAG) Pipeline for Document Q&A\n",
        "\n",
        "This notebook demonstrates a basic Retrieval Augmented Generation (RAG) pipeline. RAG is a technique that combines the power of large language models (LLMs) with a retrieval system to provide more accurate and contextually relevant answers to user queries.\n",
        "\n",
        "In this pipeline, we will:\n",
        "1.  Load and preprocess documents.\n",
        "2.  Split documents into smaller chunks.\n",
        "3.  Generate embeddings for these chunks.\n",
        "4.  Store the embeddings in a vector database (Pinecone).\n",
        "5.  Given a user query, retrieve relevant chunks from the database.\n",
        "6.  Use a language model (Groq) to generate an answer based on the retrieved context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiAhPfw25VuR"
      },
      "source": [
        "# **SETTING UP**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbbRzNrqYMXE"
      },
      "outputs": [],
      "source": [
        "# Installing necessary dependencies for the RAG pipeline.\n",
        "# pinecone: for the vector database\n",
        "# sentence-transformers: for generating embeddings\n",
        "# pymupdf: for loading PDF documents\n",
        "# langchain: a framework for developing applications powered by language models\n",
        "# langchain-community: community contributed LangChain components\n",
        "# langdetect: for detecting the language of the text\n",
        "# torch: a deep learning framework, used by sentence-transformers\n",
        "# groq: for the Language Model (LLM) used for generating answers\n",
        "!pip install pinecone sentence-transformers pymupdf langchain -q\n",
        "!pip install -U langchain-community langdetect -q\n",
        "!pip install torch -q\n",
        "!pip install groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQOdP75VYSxL"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries and modules.\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "import os\n",
        "from pinecone import Pinecone\n",
        "import re\n",
        "import unicodedata\n",
        "from langdetect import detect\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "from langchain.document_loaders import TextLoader, PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\n",
        "from groq import Groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpzUA0mYYTeo"
      },
      "outputs": [],
      "source": [
        "# Setting up Pinecone connection.\n",
        "# Replace with your actual Pinecone API Key and Environment.\n",
        "PINECONE_API_KEY = \"Your Pinecone API Key\"\n",
        "PINECONE_ENV = \"us-east-1\"\n",
        "index_name = \"index1\" # Name of your Pinecone index\n",
        "\n",
        "# Initialize Pinecone client.\n",
        "pinecone_client = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENV)\n",
        "\n",
        "# Connect to the specified Pinecone index.\n",
        "index_bge = pinecone_client.Index(name=index_name)\n",
        "\n",
        "# Set Pinecone API key as an environment variable (sometimes required by libraries).\n",
        "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wj6sy8gNYTbH"
      },
      "outputs": [],
      "source": [
        "# Initialize Groq client for the Language Model (LLM).\n",
        "# Replace with your actual Groq API Key.\n",
        "GROQ_API_KEY = \"Your Groq API Key\"\n",
        "client = Groq(api_key=GROQ_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rZMyMXtUYTY9"
      },
      "outputs": [],
      "source": [
        "# Determine the device to use for the Sentence Transformer model (GPU if available, otherwise CPU).\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load the Sentence Transformer model (BAAI/bge-m3) for generating embeddings.\n",
        "# This model is chosen for its good performance on various tasks.\n",
        "models = {\n",
        "    \"bge-m3\": SentenceTransformer(\"BAAI/bge-m3\", device=device)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpK1KXCjYjK3"
      },
      "outputs": [],
      "source": [
        "# --- Helper Functions ---\n",
        "\n",
        "# Function to clean text: normalize Unicode, remove extra whitespace.\n",
        "def clean_text(text):\n",
        "    text = unicodedata.normalize(\"NFKC\", text) # Normalize Unicode characters\n",
        "    text = re.sub(r'\\s+', ' ', text) # Replace multiple whitespace characters with a single space\n",
        "    text = text.strip() # Remove leading/trailing whitespace\n",
        "    return text\n",
        "\n",
        "# Function to preprocess documents: clean text, detect language, add metadata.\n",
        "def preprocess_documents(documents, filename):\n",
        "    cleaned_docs = []\n",
        "    for i, doc in enumerate(documents):\n",
        "        text = clean_text(doc.page_content) # Clean the document content\n",
        "        try:\n",
        "            lang = detect(text) # Detect the language of the text\n",
        "        except:\n",
        "            lang = \"unknown\" # Handle potential errors in language detection\n",
        "\n",
        "        doc.page_content = text # Update document content with cleaned text\n",
        "        doc.metadata[\"language\"] = lang # Add detected language to metadata\n",
        "        doc.metadata[\"source\"] = filename # Add source filename to metadata\n",
        "        doc.metadata[\"chunk_index\"] = i # Add chunk index to metadata\n",
        "\n",
        "        if \"page\" not in doc.metadata:\n",
        "            doc.metadata[\"page\"] = i # Add page number if not already present (e.g., for PDFs)\n",
        "\n",
        "        cleaned_docs.append(doc)\n",
        "    return cleaned_docs\n",
        "\n",
        "# Function to upload embeddings to Pinecone.\n",
        "def upload_embeddings(chunks, model_name, model, index, chunking_type):\n",
        "    batch_size = 200 # Define batch size for uploading vectors to Pinecone\n",
        "    for i in range(0, len(chunks), batch_size):\n",
        "        batch_chunks = chunks[i:i + batch_size]\n",
        "        texts = [c.page_content for c in batch_chunks]\n",
        "        # Encode the text chunks into embeddings using the specified model.\n",
        "        embeddings = model.encode(texts, show_progress_bar=False)\n",
        "\n",
        "        vectors = []\n",
        "        # Prepare vectors for upserting to Pinecone.\n",
        "        for j, embedding in enumerate(embeddings):\n",
        "            chunk_id = f\"{model_name}_{chunking_type}_chunk_{i + j}\" # Create a unique ID for each vector\n",
        "            metadata = batch_chunks[j].metadata.copy() # Copy existing metadata\n",
        "            metadata[\"text\"] = batch_chunks[j].page_content # Add the original text chunk to metadata\n",
        "            metadata[\"chunking\"] = chunking_type # Add chunking type to metadata\n",
        "            vectors.append((chunk_id, embedding.tolist(), metadata)) # Create the vector tuple (ID, embedding, metadata)\n",
        "\n",
        "        # Upsert (insert or update) the vectors to the Pinecone index.\n",
        "        index.upsert(vectors=vectors)\n",
        "        print(f\"âœ… Uploaded {len(vectors)} vectors to {model_name} ({chunking_type})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fWfIfdC5lWX"
      },
      "source": [
        "**Documents**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOO7T7u-YjE_"
      },
      "outputs": [],
      "source": [
        "# Define the list of documents to be processed.\n",
        "# Make sure these files exist in your Colab environment or Google Drive.\n",
        "docs = ['sample1.pdf', 'sample2.txt']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QNxjdiw_CG3"
      },
      "source": [
        "# **Chunking**\n",
        "This section demonstrates different methods for splitting documents into smaller pieces (chunks). Chunking is essential for managing the input size for embedding models and LLMs, and for improving the relevance of retrieved information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcMNOpbeseu8"
      },
      "outputs": [],
      "source": [
        "# Initialize lists to store chunks from all documents using different strategies.\n",
        "char_chunks_all = [] # Chunks created using fixed-length character splitting\n",
        "token_chunks_all = [] # Chunks created using token-based splitting\n",
        "sentence_chunks_all = [] # Chunks created using sentence-aware splitting\n",
        "\n",
        "# Process each document in the list.\n",
        "for doc_path in docs:\n",
        "    filename = os.path.basename(doc_path) # Get the filename from the path\n",
        "\n",
        "    # Load the document based on its file extension.\n",
        "    if doc_path.endswith(\".txt\"):\n",
        "        loader = TextLoader(doc_path)\n",
        "    elif doc_path.endswith(\".pdf\"):\n",
        "        loader = PyMuPDFLoader(doc_path)\n",
        "    else:\n",
        "        print(f\"Skipping unsupported file: {doc_path}\")\n",
        "        continue # Skip to the next file if the extension is not supported\n",
        "\n",
        "    # Load the raw documents.\n",
        "    raw_docs = loader.load()\n",
        "    # Preprocess the documents (clean text, add metadata).\n",
        "    documents = preprocess_documents(raw_docs, filename)\n",
        "\n",
        "    # --- Chunking Strategies ---\n",
        "\n",
        "    # Fixed-length character chunking: Splits text into chunks of a fixed character size with overlap.\n",
        "    char_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "    char_chunks = char_splitter.split_documents(documents)\n",
        "    char_chunks_all.extend(char_chunks) # Add these chunks to the combined list\n",
        "\n",
        "    # Token-based chunking: Splits text into chunks based on a fixed number of tokens with overlap.\n",
        "    # This is often preferred for models that work with tokens.\n",
        "    token_splitter = SentenceTransformersTokenTextSplitter(tokens_per_chunk=256, chunk_overlap=50)\n",
        "    token_chunks = token_splitter.split_documents(documents)\n",
        "    token_chunks_all.extend(token_chunks) # Add these chunks to the combined list\n",
        "\n",
        "    # Sentence-aware chunking: Splits text based on common sentence separators, aiming to keep sentences intact.\n",
        "    sentence_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=400, # Target chunk size\n",
        "        chunk_overlap=80, # Overlap between chunks\n",
        "        separators=[\"\\n\\n\", \"\\n\", \". \", \"? \", \"! \", \" \"] # Characters to use as split points\n",
        "    )\n",
        "    sentence_chunks = sentence_splitter.split_documents(documents)\n",
        "    sentence_chunks_all.extend(sentence_chunks) # Add these chunks to the combined list\n",
        "\n",
        "\n",
        "# Print the total number of chunks generated by each strategy.\n",
        "print(\"âœ… Total char chunks:\", len(char_chunks_all))\n",
        "print(\"âœ… Total token chunks:\", len(token_chunks_all))\n",
        "print(\"âœ… Total sentence chunks:\", len(sentence_chunks_all))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Euw-pdns_IzJ"
      },
      "source": [
        "# **Upload Embeddings**\n",
        "This section handles the process of generating embeddings for the document chunks and uploading them to the Pinecone vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIEP1nm1Y4N0"
      },
      "outputs": [],
      "source": [
        "# Upload embeddings for the chunks to Pinecone.\n",
        "# The upload_embeddings function handles batching and upserting to the index.\n",
        "\n",
        "# Upload token chunks embeddings.\n",
        "upload_embeddings(token_chunks_all, \"bge-m3\", models[\"bge-m3\"], index_bge, chunking_type=\"token\")\n",
        "\n",
        "# Upload char chunks embeddings.\n",
        "upload_embeddings(char_chunks_all, \"bge-m3\", models[\"bge-m3\"], index_bge, chunking_type=\"char\")\n",
        "\n",
        "# Upload sentence chunks embeddings.\n",
        "upload_embeddings(sentence_chunks_all, \"bge-m3\", models[\"bge-m3\"], index_bge, chunking_type=\"sentence\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ge0rpqZa_X9K"
      },
      "source": [
        "# **Querying & Answer**\n",
        "This section covers how a user query is processed to find relevant information in the Pinecone index and then used by a Language Model to generate an answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5xPuyzGZTAm"
      },
      "outputs": [],
      "source": [
        "# --- Querying and Answer Generation Functions ---\n",
        "\n",
        "# Function to search Pinecone for relevant chunks based on a query.\n",
        "def search_pinecone(query, model, index, top_k=10, chunking_type=None):\n",
        "    from langdetect import detect # Import langdetect here if not imported globally or if preferred locally\n",
        "\n",
        "    try:\n",
        "        query_lang = detect(query) # Detect the language of the query\n",
        "    except:\n",
        "        query_lang = \"unknown\" # Handle potential errors\n",
        "\n",
        "    # Encode the query into an embedding using the specified model.\n",
        "    query_embedding = model.encode([query], convert_to_numpy=True)[0]\n",
        "\n",
        "    # Define a filter for the Pinecone search to only include results in the same language as the query.\n",
        "    filter = {\"language\": {\"$eq\": query_lang}}\n",
        "\n",
        "    # If a chunking type is specified, add it to the filter to search only within that type of chunks.\n",
        "    if chunking_type:\n",
        "        filter[\"chunking\"] = {\"$eq\": chunking_type}\n",
        "\n",
        "    # Perform the similarity search in the Pinecone index.\n",
        "    results = index.query(\n",
        "        vector=query_embedding.tolist(), # The query embedding\n",
        "        top_k=top_k, # The number of top similar results to retrieve\n",
        "        include_metadata=True, # Include the metadata stored with each vector\n",
        "        filter=filter # Apply the language and chunking type filter\n",
        "    )\n",
        "\n",
        "    # Print the details of the retrieved matches.\n",
        "    print(\"--- Pinecone Search Results ---\")\n",
        "    for i, match in enumerate(results['matches']):\n",
        "        print(f\"\\nðŸ”¹ Match #{i+1}\")\n",
        "        print(f\"Score: {match['score']:.4f}\") # Similarity score\n",
        "        print(f\"Language: {match['metadata'].get('language', 'unknown')}\") # Language of the chunk\n",
        "        print(f\"Chunking: {match['metadata'].get('chunking', 'N/A')}\") # Chunking strategy used\n",
        "        print(f\"Source: {match['metadata'].get('source', 'N/A')}\") # Source document\n",
        "        print(f\"Text: {match['metadata']['text']}\") # The actual text of the chunk\n",
        "    print(\"-----------------------------\")\n",
        "\n",
        "    return results # Return the search results\n",
        "\n",
        "# Function to generate an answer using Groq based on retrieved context and the original query.\n",
        "def generate_answer_groq(matches, query):\n",
        "    # Combine the text from the retrieved chunks to form the context for the LLM.\n",
        "    context = \"\\n\\n\".join([match['metadata']['text'] for match in matches])\n",
        "\n",
        "    # Use the Groq client to get a completion from the LLM.\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"llama3-70b-8192\", # Specify the LLM model to use\n",
        "        messages=[\n",
        "            # System message to instruct the LLM on how to behave.\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert environmental researcher. Based on the following context extracted from scientific papers, provide a clear, well-structured, and thoughtful answer to the question below. Avoid bullet points unless necessary. Combine the information across sources, avoid redundancy, and make the answer sound like it was written by a human expert synthesizing multiple studies.\"},\n",
        "            # User message containing the context and the question.\n",
        "            {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {query}\"}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content # Return the generated answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "N4PR8M-EZWGF",
        "outputId": "f2c54183-3b87-4905-9291-f8672e0eb127"
      },
      "outputs": [],
      "source": [
        "# --- Example Usage ---\n",
        "\n",
        "# Define the user query.\n",
        "query = \"Add your query here\"\n",
        "# Generate the embedding for the query.\n",
        "query_embedding = models[\"bge-m3\"].encode(query, normalize_embeddings=True)\n",
        "\n",
        "# Search Pinecone for relevant chunks based on the query.\n",
        "# We search across all chunking types by not specifying 'chunking_type'.\n",
        "results = search_pinecone(query, model=models[\"bge-m3\"], index=index_bge, top_k=5, chunking_type=None)\n",
        "\n",
        "# Generate an answer using Groq based on the retrieved chunks.\n",
        "answer = generate_answer_groq(results['matches'], query)\n",
        "\n",
        "# Print the final generated answer.\n",
        "print(\"\\nðŸ’¡ Final Answer:\", answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Suggestions & Project Ideas\n",
        "\n",
        "Here are some ideas and directions to help guide you:\n",
        "\n",
        "---\n",
        "\n",
        "## Project Ideas\n",
        "\n",
        "- **Build a Custom Chatbot:**  \n",
        "  Use your own notes, textbooks, or research papers as the document source and create a chatbot that answers questions about them.\n",
        "\n",
        "- **Domain-Specific Q&A:**  \n",
        "  Try using this pipeline for other domainsâ€”medicine, law, history, or any subject youâ€™re interested in.\n",
        "\n",
        "- **Web App Interface:**  \n",
        "  Deploy your RAG pipeline as a web app using Streamlit or Gradio.\n",
        "\n",
        "- **Experiment with Different Models:**  \n",
        "  Swap out the embedding model or LLM (try OpenAI, Gemini, or open-source models) and compare results.\n",
        "\n",
        "- **Multilingual Support:**  \n",
        "  Add support for documents and queries in different languages.\n",
        "\n",
        "---\n",
        "\n",
        "## What to Learn Next\n",
        "\n",
        "- **Prompt Engineering:**  \n",
        "  Learn how to craft better prompts to get more accurate and useful responses from LLMs.\n",
        "\n",
        "- **Fine-tuning LLMs:**  \n",
        "  Explore how to fine-tune language models on your own data for improved performance.\n",
        "\n",
        "- **Evaluation Techniques:**  \n",
        "  Study how to evaluate the quality and relevance of LLM-generated answers.\n",
        "\n",
        "- **Vector Databases:**  \n",
        "  Dive deeper into how vector search works and try other vector DBs.\n",
        "\n",
        "- **LLM Internals:**  \n",
        "  Learn about the architecture of transformers, attention mechanisms, and how LLMs are trained.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
